{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = 'experiments/cfgs/gans/mnist.yml'\n",
    "DATASETS = ['mnist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/drajwer/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.gan import MnistDefenseGAN, FmnistDefenseDefenseGAN, \\\n",
    "    CelebADefenseGAN\n",
    "from utils.config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import zipfile\n",
    "import argparse\n",
    "import requests\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from six.moves import urllib\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Download dataset for DCGAN.')\n",
    "parser.add_argument('datasets', metavar='N', type=str, nargs='+', choices=['celebA', 'lsun', 'mnist','f-mnist'],\n",
    "           help='name of dataset to download [celebA, lsun, mnist, fmnist]')\n",
    "\n",
    "def download(url, dirpath):\n",
    "  filename = url.split('/')[-1]\n",
    "  filepath = os.path.join(dirpath, filename)\n",
    "  u = urllib.request.urlopen(url)\n",
    "  f = open(filepath, 'wb')\n",
    "  filesize = int(u.headers[\"Content-Length\"])\n",
    "  print(\"Downloading: %s Bytes: %s\" % (filename, filesize))\n",
    "\n",
    "  downloaded = 0\n",
    "  block_sz = 8192\n",
    "  status_width = 70\n",
    "  while True:\n",
    "    buf = u.read(block_sz)\n",
    "    if not buf:\n",
    "      print('')\n",
    "      break\n",
    "    else:\n",
    "      print('', end='\\r')\n",
    "    downloaded += len(buf)\n",
    "    f.write(buf)\n",
    "    status = ((\"[%-\" + str(status_width + 1) + \"s] %3.2f%%\") %\n",
    "      ('=' * int(float(downloaded) / filesize * status_width) + '>', downloaded * 100. / filesize))\n",
    "    print(status, end='')\n",
    "    sys.stdout.flush()\n",
    "  f.close()\n",
    "  return filepath\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "  URL = \"https://docs.google.com/uc?export=download\"\n",
    "  session = requests.Session()\n",
    "\n",
    "  response = session.get(URL, params={ 'id': id }, stream=True,verify=False)\n",
    "  token = get_confirm_token(response)\n",
    "\n",
    "  if token:\n",
    "    params = { 'id' : id, 'confirm' : token }\n",
    "    response = session.get(URL, params=params, stream=True,verify=False)\n",
    "\n",
    "  save_response_content(response, destination)\n",
    "\n",
    "def get_confirm_token(response):\n",
    "  for key, value in response.cookies.items():\n",
    "    if key.startswith('download_warning'):\n",
    "      return value\n",
    "  return None\n",
    "\n",
    "def save_response_content(response, destination, chunk_size=32*1024):\n",
    "  total_size = int(response.headers.get('content-length', 0))\n",
    "  with open(destination, \"wb\") as f:\n",
    "    for chunk in tqdm(response.iter_content(chunk_size), total=total_size,\n",
    "              unit='B', unit_scale=True, desc=destination):\n",
    "      if chunk: # filter out keep-alive new chunks\n",
    "        f.write(chunk)\n",
    "\n",
    "def unzip(filepath):\n",
    "  print(\"Extracting: \" + filepath)\n",
    "  dirpath = os.path.dirname(filepath)\n",
    "  with zipfile.ZipFile(filepath) as zf:\n",
    "    zf.extractall(dirpath)\n",
    "  os.remove(filepath)\n",
    "\n",
    "def download_celeb_a(dirpath):\n",
    "  data_dir = 'celebA'\n",
    "  if os.path.exists(os.path.join(dirpath, data_dir)):\n",
    "    print('Found Celeb-A - skip')\n",
    "    return\n",
    "\n",
    "  filename, drive_id  = \"img_align_celeba.zip\", \"0B7EVK8r0v71pZjFTYXZWM3FlRnM\"\n",
    "  save_path = os.path.join(dirpath, filename)\n",
    "\n",
    "  if os.path.exists(save_path):\n",
    "    print('[*] {} already exists'.format(save_path))\n",
    "  else:\n",
    "    download_file_from_google_drive(drive_id, save_path)\n",
    "\n",
    "  zip_dir = ''\n",
    "  with zipfile.ZipFile(save_path) as zf:\n",
    "    zip_dir = zf.namelist()[0]\n",
    "    zf.extractall(dirpath)\n",
    "  os.remove(save_path)\n",
    "  os.rename(os.path.join(dirpath, zip_dir), os.path.join(dirpath, data_dir))\n",
    "\n",
    "def _list_categories(tag):\n",
    "  url = 'http://lsun.cs.princeton.edu/htbin/list.cgi?tag=' + tag\n",
    "  f = urllib.request.urlopen(url)\n",
    "  return json.loads(f.read())\n",
    "\n",
    "def _download_lsun(out_dir, category, set_name, tag):\n",
    "  url = 'http://lsun.cs.princeton.edu/htbin/download.cgi?tag={tag}' \\\n",
    "      '&category={category}&set={set_name}'.format(**locals())\n",
    "  print(url)\n",
    "  if set_name == 'test':\n",
    "    out_name = 'test_lmdb.zip'\n",
    "  else:\n",
    "    out_name = '{category}_{set_name}_lmdb.zip'.format(**locals())\n",
    "  out_path = os.path.join(out_dir, out_name)\n",
    "  cmd = ['curl', url, '-o', out_path]\n",
    "  print('Downloading', category, set_name, 'set')\n",
    "  subprocess.call(cmd)\n",
    "\n",
    "def download_lsun(dirpath):\n",
    "  data_dir = os.path.join(dirpath, 'lsun')\n",
    "  if os.path.exists(data_dir):\n",
    "    print('Found LSUN - skip')\n",
    "    return\n",
    "  else:\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "  tag = 'latest'\n",
    "  categories = ['bedroom']\n",
    "\n",
    "  for category in categories:\n",
    "    _download_lsun(data_dir, category, 'train', tag)\n",
    "    _download_lsun(data_dir, category, 'val', tag)\n",
    "  _download_lsun(data_dir, '', 'test', tag)\n",
    "\n",
    "def download_mnist(dirpath):\n",
    "  data_dir = os.path.join(dirpath, 'mnist')\n",
    "  if os.path.exists(data_dir):\n",
    "    print('Found MNIST - skip')\n",
    "    return\n",
    "  else:\n",
    "    os.mkdir(data_dir)\n",
    "  url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "  file_names = ['train-images-idx3-ubyte.gz',\n",
    "                'train-labels-idx1-ubyte.gz',\n",
    "                't10k-images-idx3-ubyte.gz',\n",
    "                't10k-labels-idx1-ubyte.gz']\n",
    "  for file_name in file_names:\n",
    "    url = (url_base+file_name).format(**locals())\n",
    "    print(url)\n",
    "    out_path = os.path.join(data_dir,file_name)\n",
    "    cmd = ['curl', url, '-o', out_path]\n",
    "    print('Downloading ', file_name)\n",
    "    subprocess.call(cmd)\n",
    "    cmd = ['gzip', '-d', out_path]\n",
    "    print('Decompressing ', file_name)\n",
    "    subprocess.call(cmd)\n",
    "\n",
    "\n",
    "def download_fmnist(dirpath):\n",
    "  data_dir = os.path.join(dirpath, 'f-mnist')\n",
    "  if os.path.exists(data_dir):\n",
    "    print('Found F-MNIST - skip')\n",
    "    return\n",
    "  else:\n",
    "    os.mkdir(data_dir)\n",
    "  url_base = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'\n",
    "  file_names = ['train-images-idx3-ubyte.gz',\n",
    "                'train-labels-idx1-ubyte.gz',\n",
    "                't10k-images-idx3-ubyte.gz',\n",
    "                't10k-labels-idx1-ubyte.gz']\n",
    "  for file_name in file_names:\n",
    "    url = (url_base+file_name).format(**locals())\n",
    "    print(url)\n",
    "    out_path = os.path.join(data_dir,file_name)\n",
    "    cmd = ['curl', url, '-o', out_path]\n",
    "    print('Downloading ', file_name)\n",
    "    subprocess.call(cmd)\n",
    "    cmd = ['gzip', '-d', out_path]\n",
    "    print('Decompressing ', file_name)\n",
    "    subprocess.call(cmd)\n",
    "\n",
    "def prepare_data_dir(path = './data'):\n",
    "  if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  prepare_data_dir()\n",
    "\n",
    "  if any(name in DATASETS for name in ['CelebA', 'celebA', 'celebA']):\n",
    "    download_celeb_a('./data')\n",
    "  if 'mnist' in DATASETS:\n",
    "    download_mnist('./data')\n",
    "  if 'fmnist' in DATASETS:\n",
    "    download_fmnist('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(cfg, *args):\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "    ds_gan = {\n",
    "        'mnist': MnistDefenseGAN, 'f-mnist': FmnistDefenseDefenseGAN,\n",
    "        'celeba': CelebADefenseGAN,\n",
    "    }\n",
    "    GAN = ds_gan[FLAGS.dataset_name]\n",
    "\n",
    "    gan = GAN(cfg=cfg, test_mode=not FLAGS.is_train)\n",
    "\n",
    "    if FLAGS.is_train:\n",
    "        gan.train()\n",
    "\n",
    "    if FLAGS.train_encoder:\n",
    "        gan.load(checkpoint_dir=FLAGS.init_path)\n",
    "        gan.train(phase='just_enc')\n",
    "\n",
    "    if FLAGS.save_recs:\n",
    "        gan.reconstruct_dataset(ckpt_path=FLAGS.init_path,\n",
    "                                max_num=FLAGS.max_num)\n",
    "\n",
    "    if FLAGS.test_generator:\n",
    "        gan.load_generator(ckpt_path=FLAGS.init_path)\n",
    "        gan.sess.run(gan.global_step.initializer)\n",
    "        gan.generate_image(iteration=0)\n",
    "\n",
    "    if FLAGS.test_batch:\n",
    "        gan.test_batch()\n",
    "\n",
    "    if FLAGS.save_ds:\n",
    "        gan.save_ds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Note: The load_config() call will convert all the parameters that are defined in\n",
    "    # experiments/config files into FLAGS.param_name and can be passed in from command line.\n",
    "    # arguments : python train.py --cfg <config_path> --<param_name> <param_value>\n",
    "    cfg = load_config(CONFIG_FILE)\n",
    "    flags = tf.app.flags\n",
    "\n",
    "    flags.DEFINE_boolean(\"is_train\", False,\n",
    "                         \"True for training, False for testing. [False]\")\n",
    "    flags.DEFINE_boolean(\"save_recs\", False,\n",
    "                         \"True for saving reconstructions. [False]\")\n",
    "    flags.DEFINE_boolean(\"debug\", False,\n",
    "                         \"True for debug. [False]\")\n",
    "    flags.DEFINE_boolean(\"test_generator\", False,\n",
    "                         \"True for generator samples. [False]\")\n",
    "    flags.DEFINE_boolean(\"test_decoder\", False,\n",
    "                         \"True for decoder samples. [False]\")\n",
    "    flags.DEFINE_boolean(\"test_again\", False,\n",
    "                         \"True for not using cache. [False]\")\n",
    "    flags.DEFINE_boolean(\"test_batch\", False,\n",
    "                         \"True for visualizing the batches and labels. [False]\")\n",
    "    flags.DEFINE_boolean(\"save_ds\", False,\n",
    "                         \"True for saving the dataset in a pickle file. [\"\n",
    "                         \"False]\")\n",
    "    flags.DEFINE_boolean(\"tensorboard_log\", True, \"True for saving \"\n",
    "                                                  \"tensorboard logs. [True]\")\n",
    "    flags.DEFINE_boolean(\"train_encoder\", False,\n",
    "                         \"Add an encoder to a pretrained model. [\"\n",
    "                         \"False]\")\n",
    "    flags.DEFINE_boolean(\"init_with_enc\", False,\n",
    "                         \"Initializes the z with an encoder, must run \"\n",
    "                         \"--train_encoder first. [False]\")\n",
    "    flags.DEFINE_integer(\"max_num\", -1,\n",
    "                         \"True for saving the dataset in a pickle file [\"\n",
    "                         \"False]\")\n",
    "    flags.DEFINE_string(\"init_path\", None, \"Checkpoint path. [None]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-42b51410b3ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmain_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/drajwer/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#] MnistDefenseGAN.dataset_name is set to mnist.\n",
      "[#] MnistDefenseGAN.batch_size is set to 50.\n",
      "[#] MnistDefenseGAN.use_bn is set to False.\n",
      "[#] MnistDefenseGAN.test_batch_size is set to 20.\n",
      "[#] MnistDefenseGAN.mode is set to wgan-gp.\n",
      "[#] MnistDefenseGAN.gradient_penalty_lambda is set to 10.0.\n",
      "[#] MnistDefenseGAN.train_iters is set to 200000.\n",
      "[#] MnistDefenseGAN.critic_iters is set to 5.\n",
      "[#] MnistDefenseGAN.latent_dim is set to 128.\n",
      "[#] MnistDefenseGAN.net_dim is set to 64.\n",
      "[#] MnistDefenseGAN.input_transform_type is set to 0.\n",
      "[#] MnistDefenseGAN.debug is set to False.\n",
      "[#] MnistDefenseGAN.rec_iters is set to 200.\n",
      "[#] MnistDefenseGAN.image_dim is set to [28, 28, 1].\n",
      "[#] MnistDefenseGAN.rec_rr is set to 10.\n",
      "[#] MnistDefenseGAN.rec_lr is set to 10.0.\n",
      "[#] MnistDefenseGAN.test_again is set to False.\n",
      "[-] MnistDefenseGAN.loss_type is not set.\n",
      "[#] MnistDefenseGAN.loss_type is set to None.\n",
      "[-] MnistDefenseGAN.attribute is not set.\n",
      "[#] MnistDefenseGAN.attribute is set to None.\n",
      "[#] MnistDefenseGAN.tensorboard_log is set to True.\n",
      "[#] MnistDefenseGAN.output_dir is set to output.\n",
      "[#] MnistDefenseGAN.num_gpus is set to 1.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "\n",
    "main_cfg = lambda x: main(cfg, x)\n",
    "tf.app.run(main=main_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
